---
title: "RNA-Seq"
output: pdf_document
---

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NULL)
```

```{r, echo=F,include=FALSE,warning=FALSE}
# Load required packages: 
library(limma)
library(edgeR)
library(gplots)
library(RColorBrewer)

library(BiocGenerics)
library(parallel)
library(Biobase)
```


# Analysis of RNA-Seq Data

## Statistical Methodologies and RNA-Seq Data

Over the last two decades, many different statistical methods have been introduced for the specific purpose of application in differential gene expression analysis of RNA-Seq data. These methods have been designed to assist in overcoming the challenges that integer count data present in this setting. Differential gene expression tools that have been proven to perform very well are `edgeR` (@edger), `DESeq/DESeq2` (@deseq, @deseq2) and `limma-voom` (@law), which can all be used in `R`. These various procedures all use a wide range of statistical techniques in order to ensure detection of many truly differentially expressed genes even when data from a limited set of samples is available. The strategies applied in each case have notable similarities. Regression-based models are used to estimate changes in gene expression levels for each gene while a hypothesis test is carried out to determine if this change in expression between two conditions of interest should be considered significant. However, the tools mentioned can be clearly separated into two distinct groups: 

1) modelling of the count data directly takes place in order to make inferences: `edgeR` and `DESeq/DESeq2`
2) normal-based methodologies are implemented upon transformation of the counts:  `limma-voom`


Due to the fact that there has been much focus on the use of normal linear models for microarray data with `limma`, it is of interest to investigate how a similar approach may be taken with data generated by RNA-Seq technologies. Therefore, the latter of these two methods will be explored in much detail. This will include examining how unequal variabilities across different count sizes is managed. That said, the use of count-based modelling techniques along with their limitations will be first briefly discussed.

\  

**Use of count distributions and their limitations:**

RNA-Seq data takes the form of integer counts which represent the number of sequence reads mapped to a specific gene. Because of this, modelling this data appropriately using count distributions has been an area of much research. The Poisson distribution is a popular distribution used to model count variables which possesses the convenient characteristic of its mean and variance being equivalent. However, it would be most naive to assume that this distribution is suitable for modelling purposes here. Biological replicates have been shown to introduce greater variability, resulting in a distribution which takes a similar shape to that of an overdispersed Poisson rather than the basic Poisson distribution. This overdispersion can be expressed using a negative binomial distribution, a distribution considered to be similar to the Poisson distribution but in which the variance is permitted to have a greater magnitude than the mean. Both `edgeR` and `DESeq/DESeq2` procedures incorporate the fitting of these negative binomial models to the observed count data so that variability of both technical and biological origin can be accounted for. 

Using a negative binomial model necessitates the estimation of two parameters, both the mean and the variance, from the dataset. The availability of merely a few replicates for most RNA-Seq experiments makes accurate estimation of these parameters difficult. Both `DESeq/DESeq2` and `edgeR` have aimed to improve the reliability of its inferences through information borrowing across genes. With `DESeq`, it is presumed that all genes share the same parameter of dispersion for which estimation can occur with the use of a conditional maximum likelihood method involving the entire dataset. Clearly, this can often be quite a restrictive approach. An empirical Bayes method based on weighted conditional likelihood was introduced by @robsmy which was designed to allow gene-wise dispersion estimates. These dispersion estimates for each individual gene are shrunk towards the common dispersion, moderating the degree of overdispersion across genes. 

It is known that the use of count distributions, such as the negative binomial, can have many limitations in comparison to normal distributions, as a result of the mathematical intractability of this type of probability distribution. Hypothesis testing with these distributions is often based on tests such as the Wald test and likelihood ratio test which are only asymptotically valid. A non-asymptotic exact test was introduced by @robsmy. However, this test assumes that the negative binomial dispersion parameters are equivalent for the two groups of interest. In fact, many of the negative binomial methods are inclined to regard the estimated dispersions as known parameters, failing to take into account uncertainty in estimation. 
 
It was noted by @soneson that the presence of an outlier in the dataset could result in a considerable increase in the type I error rate of methods which are based on the use of negative binomial modelling. Additionally, for samples sizes which were large in number, `DESeq2` was seen to act most conservatively. On the other hand, `edgeR` was regularly overly liberal as it deemed many genes as significant, including those that were truly differentially expressed as well as a great amount which weren't differentially expressed in reality. It was concluded by @soneson that these various observations were to be explained by the different ways in which dispersion parameters were estimated.


Prior to the introduction of RNA-Seq technologies, a large body of statistical tools had been devised in order to perform detailed analyses on microarray data. However, many of these rely on the data being normally distributed such as incorporating quality weighting into the analyses, conduction of gene set testing and use of random effects. Unfortunately, these methodologies are inaccessible to RNA-Seq data when it is modelled using count distributions. The prospect of the extension of RNA-Seq analyses to include the techniques mentioned above motivated the establishment of the `limma-voom` procedure by @law.



## A Local RNA-Seq Mouse Dataset

In order to investigate the steps which must be taken in the analysis of RNA-Seq data, a local RNA-Seq dataset has been obtained from research conducted to support studies in breast cancer in conjunction with the National Breast Cancer Research Institute, NBCRI. The samples used in this experiment have been extracted from murine cells. The dataset has been found to be quite limited, comprising of a mere total of $4$ samples and $1,908$ genes. It is also unfortunate that many of these genes have very low counts across all samples as will be seen. Nevertheless, the dataset will prove useful for illustrative purposes. Two replicates of each condition are provided. Count data has originated from secreted extracellular vesicles and from cells. Thus, the question of interest which may be proposed here is to simply identify which genes are differentially expressed between these two groups, given the data at hand.  

The integer count data which has been generated for the first $4$ genes is displayed directly below. Genes have been annotated in the form `GeneIDX` while the four samples may be identified as `RNA4T1A`, `RNA4T1B`, `EV4T1A` and `EV4T1B`. It should be clear that the first two samples are those derived from the cell while the second two correspond to the secreted extracellular vesicles. The sample information available for this dataset merely specified the samples' origin. 



```{r,include=FALSE}
# Read the data into R
rnaseq_data <- read.table("data/Human_Counts.txt")

count_data <- rnaseq_data[,2:17]   # only using samples from control vs cancer
rownames(count_data) <- rnaseq_data[,1]  # Gene ID numbers are included as rownames, new matrix contains only counts of 8 samples
head(count_data)
#dim(count_data) # 8 samples, 2578 genes, smaller dataset than we have dealt with previously 

# read sample info into R
sample_info <- read.delim("data/Sample_Info_Human.txt",stringsAsFactors=FALSE)
sample_info <- sample_info[2:17,]
sample_info

colnames(count_data) <- sample_info$Sample.ID ## NOTE: The col names are now same as SampleName in sampleinfo file
colSums(count_data) # different library sizes 

# work only with Serum EV control vs cancer
count_data_sub <- count_data[,9:16]
sample_info_sub <- sample_info[9:16,]
sample_info_sub


## try working with local mouse data
mouse_data <- read.table("data/Mouse_Data.txt")
mouse_count_data <- mouse_data[,2:5]
rownames(mouse_count_data) <- mouse_data[,1]
mouse_count_data
sample_info_mouse <- read.delim("data/Sample_Info_Mouse.txt")
sample_info_mouse <- sample_info_mouse[1:4,1:5]


```

```{r, echo=FALSE}
mouse_count_data[1:4,]
```


Appropriate filtration and normalisation procedures are to be performed first on the data using `edgeR`. These are necessary so that `limma-voom` can be implemented and normal linear models can be applied to this RNA-Seq dataset. 



## Filtration and Investigation of Sample Distributions  

In general, the first step in the analysis of RNA-Seq data is the removal of genes which have very low counts across all samples. As a result of small count values, these genes are incapable of showing much evidence for differential expression and thus, are likely to negatively affect certain statistical approximations that are required to be made in the analysis process. Furthermore, they add to the multiple testing burden by decreasing the power to extract genes which are truly differentially expressed between conditions.
There are many ways in which filtration of lowly expressed genes may occur. With RNA-Seq data, it is often the case that significant differences can exist in the depth to which samples have been sequenced. This gives reason as to why merely comparing counts in order to remove these genes could lead to great errors. Therefore, a common approach is to convert all counts to counts-per-million values and subsequently, define a specific threshold. 

Suppose an experiment has been performed in which a total of $n$ RNA samples have been obtained. The samples are sequenced, generating RNA-Seq libraries for each individual sample. These libraries contain recordings of the amount of reads which have been mapped to each gene for their corresponding sample. Thus, the total number of mapped reads for a particular sample is defined as its library size. This data may be represented by a matrix of counts, denoted by $r_{gi}$, for samples $i=1,\dots,n$ and genes $g=1,\dots,G$, and the library size for sample $i$ is given as: $$R_i = \sum_{g=1}^G r_{gi}.$$ Variance in library sizes indicates that the read count obtained for a given gene is not only dependent on its expression level but is also proportional to the sequencing depth of the library. Counts-per-million (*cpm*) are obtained by dividing each individual read count by its associated library size, in millions.
Equivalently, $$\text{cpm}_{gi} = \frac{r_{gi}}{R_i} \times 10^6.$$ This provides a basic measure which can be compared across all samples, even if library sizes are drastically different from one another. A basic interpretation of the *cpm* value of a particular gene, $\text{cpm}_{gi}$, is that it exhibits the number of mapped reads that would be obtained for that gene if the library size was reduced to 1 million. 


There is an enormous difference between library sizes for the two groups of this dataset. The first two samples, namely `RNA4T1A` and `RNA4T1B` have a total number of mapped reads of $951,042$ and $932,341$, respectively. In great contrast, the library sizes of the other two samples are greatly reduced with a count sum of $72,604$ for `EV4T1A` and $84,002$ for `EV4T1B`. The barplot below, in Figure \ref{fig:libraries}, displays these values. 

```{r libraries, echo=FALSE,fig.align='center',out.width='60%', fig.cap='\\label{fig:libraries}\\textit{Barplot of the library sizes of each of the four samples.}'}
# barplot of library sizes, last argument rotates the axis names
color <- brewer.pal(7,"Pastel2")
#color
barplot_colors <- c(color[3],color[3],color[1],color[1])
barplot(colSums(mouse_count_data),names=colnames(mouse_count_data),col=barplot_colors,las=2,cex.names=0.9)
title("Barplot of library sizes")
#median(colSums(mouse_count_data))
```


`edgeR`'s `cpm()` function is implemented to generate counts-per-million values. Take for instance, the count related to the first gene, `GENEID1` in the first sample, `RNA4T1A` of $52$. Its corresponding counts-per-million was obtained to be: $$\text{cpm}_{11} = \frac{r_{11}}{R_1} \times 10^6 = \frac{52}{951042} \times 10^6 \approx 58.8828.$$
Following this conversion for each observation, a *cpm* threshold must be chosen which describes each gene to be either expressed or unexpressed. In general, a good threshold is considered to be one which is equivalent to a read count of about $10$ to $15$. In this RNA-Seq dataset at hand, it was seen that library sizes cover a vast range from approximately $70,000$ to $1,000,000$. It was decided that an appropriate means of filtration would be to first compute the median library size. This was estimated to be $\approx 508,200$, allowing a rough *cpm* threshold of $\frac{12.5}{508200} \times 10^6 \approx 25$ to be defined. As there are $2$ replicates for each group, the action of retaining genes which exceed this threshold in at least $2$ samples is favoured. A similar approach to filtration of lowly expressed genes is taken by the `filterByExpr()` function found in the `edgeR` package.

This filtration process resulted in the removal of a total of $834$ genes from the dataset, leaving a remainder of $264$ which will be kept for further analysis. This set of $264$ genes are all considered to be *expressed* in at least one group. In general, it would be expected that a much larger proportion of genes would be kept. However, retaining more genes which have low count figures from our dataset by reducing the threshold could have a severely negative impact in downstream analyses such as modelling the mean-variance relationship. 


Moreover, the voom-plot displayed on the left in Figure \ref{fig:voom} provides a visual check that correct filtration has taken place. If filtering of lowly-expressed genes is insufficient, a drop in variance levels can be observed at the lower end of the expression scale due to very small counts. If this is observed, the filtering step should be returned to and  the expression threshold applied to the dataset increased. The fact that there is no evidence of a decrease in variance levels to the left of the voom-plot due to very low counts supports the threshold that has been chosen here for filtration purposes.
 

```{r,include=FALSE}
# Use cpm function to generate CPM (counts per million) values and then filter.

count_per_mil <- cpm(mouse_count_data)
count_per_mil[1:20,]

median(colSums(mouse_count_data))

# Which values in myCPM are greater than 0.5?
thresh <- count_per_mil > 25
table(rowSums(thresh)) # there are 11433 genes that have TRUES in all 12 samples
# keep genes that have at least 2 TRUES in each row of thresh
keep <- rowSums(thresh) >= 2
# subset the rows of countdata to keep more highly expressed genes

counts.keep <- mouse_count_data[keep,]
dim(counts.keep)
```



The effect of filtration can be visually represented through the production of a density plot. Currently, data is in the form of counts-per-million, which is not normally distributed. It is difficult to conduct a proper examination of the distribution of the data in this form, and thus, conversion to the log-scale is necessary. The density plot, visible in Figure \ref{fig:density} includes merely the data of the retained genes for each of the four samples.


```{r, echo=FALSE, fig.align='center',out.width='70%',include=FALSE}
# Get log2 counts per million
# Convert counts to DGEList
#y <- DGEList(counts.keep)
#logcounts <- cpm(y,log=TRUE,prior.count=0.5)


#boxplot(log_counts,xlab="",ylab="Log2 counts per million",las=2)
#abline(h=median(log_counts),col="orange",lwd=2)
#title("Boxplots of unnormalised logCPMs")
#colSums(countdata)
#mean(colSums(counts.keep))
#head(countdata)
#head(logcounts)

#dge <- DGEList(mouse_count_data)
#keep <- filterByExpr(dge, design)
#dge <- dge[keep,keep.lib.sizes=FALSE]
#log_counts <- cpm(dge,log=TRUE)
# Check distributions of samples using boxplots
#boxplot(cpm(y),xlab="", ylab="counts per million", las=2)
#boxplot(logcounts,xlab="",ylab="Log2 counts per million",las=2)
# orange horizontal line corresponding to median logCPM
#abline(h=median(logcounts),col="orange",lwd=2)

```

```{r density, echo=FALSE,fig.align='center',out.width='65%', fig.cap='\\label{fig:density}\\textit{Density plot of count-per-million values on the log-scale for each sample after filtration.}'}
# could plot densities before and after
#library(RColorBrewer)

y <- DGEList(counts.keep)
log_counts <- cpm(y,log=TRUE)
nsamples <- ncol(log_counts)
col <- brewer.pal(nsamples, "Paired")
plot(density(log_counts[,1]), col=col[1],lwd=2, ylim=c(0,0.2), las=2, main="", xlab="")
title(main=" ", xlab="Log-cpm")
for (i in 2:nsamples){
  den <- density(log_counts[,i])
  lines(den$x, den$y, col=col[i], lwd=2)
}
legend("topright", legend=colnames(log_counts), fill=col) 
```



A plot which displays the expression distribution for each sample, such as the one produced, is helpful in finding samples which may be dissimilar to others. The presence of a sample lying more to the extreme left or right of the plot would strongly emphasize that further investigation of this particular sample should take place before proceeding. With respect to this dataset, the distributions of the counts-per-million values on the log-scale are viewed to be quite similar throughout all samples. They are by no means identical in nature but great differences are not visible. 

It is important to note that the data is still considered to be *unnormalised*. Even though it is not the case here, large differences between median values for different samples is often witnessed. It would therefore be expected that if a differential expression analysis was performed on two samples for which this was an issue, almost all genes would be down-regulated in one sample while up-regulated in the other. This problem is overcome by effectively increasing or decreasing the expression of all genes in each sample to a level in which proper comparisons can take place using the normalisation procedures detailed in the next section. 

## Normalisation of Read Counts 

One of the most fundamental steps in attempting to gain information from large quantities of RNA-seq data regarding differential expression, is the processing procedures employed to make the data suitable for implementation of statistical techniques. An example of one such method is the filtration of lowly expressed genes detailed previously. However, similar to microarray data, another essential task is that of normalisation. This permits precise parameter estimation through ensuring that all samples of the experiment possess similar distributional patterns in their respective expression values. Just as with microarray data, factors besides those of biological origin, such as technical errors arising from sample preparation and the sequencing process have the potential to cause variation between samples. Furthermore, the number of mapped reads for a particular gene and sample, $r_{gi}$, tends to be influenced by the composition of the population of RNA from which the sample originates. For instance, consider an individual condition which has a large set of highly expressed genes that are distinct to that condition. As a consequence of this, it is likely that there will only be a small number of reads mapped to the other genes. Thus, if this is not taken into account, the existence of bias will be clear in the analysis results in which there may be a large number of upregulated genes when this condition is compared to another. 
 
The step of normalisation is employed to eliminate systematic technical effects, minimising the possibility for skewed analyses. In Section 3.1, the RAM normalisation procedure, which incorporates quantile normalisation to counteract the influence of technical errors on data obtained from microarrays, is discussed. However, there exist many obvious differences between the manner in which RNA-Seq data is generated and the production of microarray data. For this reason, the normalisation methods used when dealing with microarrays are not directly transferable. 

A normalisation procedure created specifically for RNA-Seq data, namely the trimmed mean of M-values normalisation method (TMM), was presented by @robosh. This empirical method was designed with the aim of eradicating composition biases across samples. Suitable scaling factors are evaluated with the assistance of the raw count data. These normalisation factors are then available for incorporation into the statistical model which has been chosen for the identification of differentially expressed genes. A description of the TMM technique is provided as follows. 

It was suggested by @robosh that robust estimation of ratios of RNA production between samples could be obtained by using the means of log-expression ratios which are both weighted and trimmed. A mean which is considered to be *trimmed* is equivalent to the average value obtained following the elimination of a specified upper and lower percentage of the data. 
The log-fold change in expression level for gene $g$ in sample $i$, relative to a particular reference sample $i'$, is defined as: $$M_{gi}^{i'}=\log_2\left( \frac{r_{gi}/R_i}{r_{gi'}/R_{i'}} \right)$$ with its absolute expression level given by: $$A_g = \frac{1}{2} \log_2(r_{gi}/R_{i}  \boldsymbol{\cdot} r_{gi'}/R_{i'})$$ for $r_{gi},r_{gi'} \ne 0$.  

In order to determine appropriate scaling factors, the TMM method first trims both the log-fold changes, M values, and the absolute expression levels, A values. The default trimming percentages of $30$% for M values and $5$% for A values are generally used. After the data has been trimmed, a weighted mean of the M values is acquired. Weighting occurs in accordance with inverse variances, with the aim of accounting for the greater variance that is shown in log-fold changes of genes with lower counts. This weighted mean is therefore intended to robustly represent the M values which were originally observed. Then, the normalisation factor for sample $i$, in which sample $i'$ is used as a reference, is explicitly computed to be:
$$\log_2 \left(TMM_i^{(i')}\right) = \frac{\sum_{g \in G^*}w^{i'}_{gi}M^{i'}_{gi}}{\sum_{g \in G^*} w_{gi}^{i'}}$$ where $$M_{gi}^{i'} = \frac{\log_2\left( r_{gi}/R_i\right)}{\log_2\left(r_{gi'}/R_{i'}\right)} \text{ and } w_{gi}^{i'} = \frac{R_i - r_{gi}}{R_ir_{gi}} + \frac{R_{i'}-r_{gi'}}{R_{i'}r_{gi'}}$$ and $r_{gi},r_{gi'} > 0$.

Instances in which $r_{gi} = 0$ or $r_{gi'} = 0$ are discarded prior to the above calculation of the scaling factor. $G^*$ is used to symbolize the collection of genes remaining after the trimming process. For the case in which data associated with several samples has been obtained, one sample is designated as the reference sample. Scaling factors are subsequently calculated for every other sample with respect to this chosen sample. It is clear from the formula given above that the normalisation factor of the sample which is being used as a reference is equivalent to $1$.

 
The `calcNormFactors()` functon in `edgeR` executes the TMM normalisation procedure, resulting in the generation of scaling factors. For symmetrical purposes, the factors are altered so that they multiply to unity across all samples.
Modification of observed library sizes occurs through multiplication by their corresponding scaling factor in order to obtain effective library sizes. It is noted that unlike many other normalisation strategies, TMM does not make any adjustments to the raw data. Instead, the outcome of its implementation is that it is these effective library sizes that will be used in downstream analyses in place of the original library sizes. As stated, TMM normalisation scales relative to one specific sample, known as the reference sample. This sample is determined by the `calcNormFactors()` function as the sample which possesses the library whose upper quartile is nearest to the mean upper quartile. 


The effect of TMM-normalisation on this local dataset is considerably mild as anticipated from inspection of the density plot in Figure \ref{fig:density}. This is evident from the magnitude of the scaling factors, which are all relatively close to 1. The scaling factors were determined by the `calcNormFactors()` function to be:

```{r,echo=FALSE}
y <- calcNormFactors(y)
```

RNA4T1A | RNA4T1B | EV4T1A | EV4T1B 
------------- | ------------- | ------------- | ------------- 
`r y$samples$norm.factors[1]` | `r y$samples$norm.factors[2]` | `r y$samples$norm.factors[3]` | `r y$samples$norm.factors[4]`


A normalisation factor less than $1$ indicates that the counts-per-million values will be scaled upwards in that sample. In the case of `RNA4T1A`, the scaling factor is noted to be approximately $0.95$. This transforms the current library size of $949,118$ into an effective library size of $949118 \times 0.95 \approx 903646$. Conversely, a factor greater than $1$ leads to downscaling the observed counts. The scaling factor computed for `EV4T1A` of roughly $1.08$ increases its library size from $72,452$ to $72452 \times 1.08 \approx 78248$.  


```{r, echo=FALSE, include=FALSE, out.width='70%'}

## NORMALISATION FOR COMPOSITION BIAS
# apply normalisation to DGEList object
y <- calcNormFactors(y)
y$samples
#mean((y$samples$lib.size)*(y$samples$norm.factors))
#doesn’t normalize the data, it just calculates normalization factors for use downstream
#y[,10:11]

# A normalization factor greater than 1 is equivalent to scaling up the library size, which is equivalent to downscaling the counts - mighnt-include diagram here -just make brief note
par(mfrow=c(1,2))
plotMD(log_counts,column = 1)
abline(h=0,col="grey")
plotMD(y,column = 1)
abline(h=0,col="grey")






#par(mfrow=c(1,2))
#plotMD(logcounts[,10:11],column = 2)
#abline(h=0,col="grey")
#plotMD(y[,10:11],column = 2)
#abline(h=0,col="grey")
# Composition bias solved - clear to see for one sample .. 
```

## Linear Modelling and Count Data: `limma-voom`

After these primary actions have been taken, attention is turned to the identification of which statistical methodologies can be applied to the dataset in order to successfully detect genes that are truly differentially expressed. As discussed previously, there has been much debate about what methods are most appropriate when analysing count data from RNA-Seq experiments. The possibility for the transformation of count data into normally distributed data with greater tractability by means of *precision weights* was introduced by @law. This approach was created in order to gain access to the wide variety of techniques which have been established for microarray data in the `limma` software package, such as quality weighting and gene set testing. The method constructed by @law was named *voom* and is implemented in `R` using the `voom()` function from `limma`. 
 
The various stages of the method may be summarized as follows: 

 1. Firstly, each individual observed read count is transformed to a log-counts-per-million (log-*cpm*) value, using the effective library sizes obtained through TMM normalisation. 
 2. Next, a linear model is fitted to these log-*cpm* values, in which the specified experimental design is taken into consideration.
 3. Application of this linear model supplies residual standard deviations for each gene. A mean-variance trend is then fitted to these standard deviations as a function of average log-count across all genes and samples. 
 4. This obtained mean-variance relationship is subsequently used to estimate the standard deviation of each log-*cpm* value as a function of its associated fitted count value. For each individual observation, the squared inverse of the estimated standard deviation becomes its own unique precision weight. 
 5. The log-*cpm* values together with their associated precision weights are ready to be analysed for evidence of differential gene expression with the `limma` package and its functions.


### Transformation: log-*cpm*s

Production of the boxplots above supports the requirement of transformation of the raw counts to log-counts-per-million following normalisation in order to make this dataset available for `limma`'s linear modelling analysis procedure, as detailed in Section 3. The distribution of the log-*cpm*s for each sample shown in Figure \ref{fig:density} indicate the possibility for these values to be treated in a similar manner to the log-intensities generated by microarray experiments. For a given gene $g$, the difference in its log-*cpm* value between two conditions of interest can be understood to be equivalent to a log-fold change in expression level. However, a crucial difference between these two forms of measurements is that it simply cannot be assumed that log-*cpm*s have constant variances. Many statistical analyses operate by assuming that all variables of the dataset possess the same variance. If this assumption that the data is homoscedastic in nature is violated, then fitting linear models directly to the data and carrying out statistical tests can lead to greatly inaccurate results. Thus, it would be incorrect to believe that the log-*cpm* values can be directly entered into the `limma` pipeline as the log-intensities were. 

First, a formal definition of the manner in which log-*cpm* quantities are obtained from the raw count data is provided as follows. Using the notation previously introduced, the corresponding log-*cpm* for a particular count of mapped reads to gene $g$ in sample $i$ may take the form: $$y_{gi} = \log_{2}\left(\frac{r_{gi}+0.5}{R_i + 1} \times 10^6\right).$$ Notice that $y_{gi}$ is not simply equal to $\log_{2}(\text{cpm}_{gi})$, with $\text{cpm}_{gi}=\frac{r_{gi}}{R_i}$. A small offset of $0.5$ is added to the count number, $r_{gi}$ in order to ensure that the log of zero will not be taken. This inclusion of an offset guarantees that no log-*cpm* values will be lost and results in a reduction of variability for those genes which still have relatively low counts. Furthermore, the library size $R_i$ endures an offset of 1 which is designed to ensure that the fraction, $\frac{r_{gi}+0.5}{R_i+1}$ is strictly between 0 and 1. With `limma`'s `voom()` function, original count values are converted to log-*cpm*s using this exact format but in which $R_i$ is the *effective* library size. 
 

Illustration of this calculation is provided with the aid of this local dataset. Take for instance the first gene, denoted by `GeneID1`. The number of mapped reads to this gene in the first sample, namely `RNA4T1A`, was observed to be $r_{11} = 52$. After normalisation using `calcNormfactors` function was carried out, the effective library size was noted to be $R_1 = 903,646$. Computation of this particular log-*cpm* value may now be performed as follows: $$y_{11}=\log_{2}\left(\frac{r_{11}+0.5}{R_1 + 1} \times 10^6\right) = \log_{2}\left(\frac{52+0.5}{903646 + 1} \times 10^6\right) \approx 5.8604.$$


### Variance of log-*cpm*s

As mentioned, it cannot be assumed that the log-*cpm* values have non-constant variances. In fact, a common characteristic of probability distributions for count data is that of heteroskedasticity. Take for example the Poisson distribution, a distribution frequently used to describe count data, in which the variance increases with the mean. A visual demonstration of the heteroscedasticity concept is obtained by plotting the mean against the standard deviation across the four samples for the log-*cpm* values of each gene, as can be seen in Figure \ref{fig:meanvar}. Existence of variability is anticipated throughout the plot. However, it is most apparent from the left-hand side of the plot that the variance seems to be much greater for lower log-*cpm* values, specifically those less than $5$. This is a clear indication of the dependence of the variance on the mean. It is of interest to investigate further what exact form the mean-variance trend of read counts on the log scale should take. 


```{r meanvar, echo=FALSE, fig.align='center',out.width='55%', fig.cap='\\label{fig:meanvar}\\textit{Plot of mean, on the x-axis, against standard deviation, on the y-axis, for each gene showing the mean-variance trend of the data.}'}
# mean-standard deviation plot
#fig.cap=
#colSums(mouse_count_data)
#head(log_counts)
#v$E[1,]
#round(y$samples$norm.factors[1],4)
msd_plot <- meanSdPlot(log_counts,ranks=FALSE,plot=FALSE)
msd_plot$gg + ylab("standard deviation")

```



An approximate quadratic mean-variance relationship for count data obtained from RNA-Seq experiments was postulated by @mccarthy. It was proposed that considering the variance formula of a mixture distribution, this relationship should take the form of $$\text{var}(r_{gi}) = \mu_{gi} + \phi_g\mu_{gi}^2$$ in which $\mu_{gi} = E(r_{gi})$ where $r_{gi}$ is the number of sequence reads mapped to gene $g$ for sample $i$ and $\sqrt{\phi_g}$ is said to represent the biological coefficient of variantion, $\text{CV}_{bio}$. However, $\text{CV}_{bio}$ can also incorporate variation resulting from technical errors such as sample preparation as well as the true biological variation that exists between samples. With knowledge that the coefficient of variation is simply the ratio of standard deviaton to the mean, division by $\mu_{gi}^2$ in the above formula permits the construction of the following: $$\text{CV}^2(r_{gi}) = \frac{1}{\mu_{gi}} + \phi_g = \text{CV}_{tech}^2 + \text{CV}_{bio}^2.$$

It is evident that $\text{CV}_{tech}^2 = \frac{1}{\mu_{gi}}$, a term arising from variation introduced by the sequencing process, is a decreasing function of the expected count size, while $\text{CV}_{bio}^2$ remains at a constant level. 
 
It was concluded by @law that through the performance of a simple linearization calculation, the standard deviation of log-*cpm* values are in fact approximately equivalent to $\text{CV}(r_{gi})$, the coefficient of variation of count size. From above, $$y_{gi} \approx \log_2\left( \frac{r_{gi}}{R_i} \times 10^6 \right) = \log_2(r_{gi}) - log_2(R_i) + 6\log_2(10).$$ Treating $R_i$, the library size, as a constant, it is easily seen that $\text{var}(y_{gi}) \approx \text{var}(\log_2(r_{gi})$. Taylor's theorem indicates that $\log_2(r_{gi}) \approx \mu_{gi} + \frac{r_{gi}-\mu_{gi}}{\mu_{gi}}$, in which $\mu_{gi}$ is of a relatively large value. Thus, $$\text{var}(y_{gi}) \approx \text{var}\left(\frac{r_{gi}}{\mu_{gi}}\right) = \frac{1}{\mu_{gi}^2}\text{var}(r_{gi}) = \frac{1}{\mu_{gi}} + \phi_g.$$

It is therefore to be expected that the standard deviation of log-*cpm*s is a steadily decreasing function of the mean count size, for counts which are of small to medium magnitude. For greater observed count sizes, demonstration of asymptotic behaviour at a certain level, which is dependent on biological variability, should be likely.


The establishment of the *voom* method by @law originated from the idea that precise modelling of mean-variance relationships is fundamental in the design of methodologies with great statistical power. It was thus proposed that correct modelling the mean-variance trend of log-cpm values at individual observation level should supersede the aim of exact specification of the probability distribution. In order to achieve this, a comprehension of the way in which the variability depends on count size, which has been discussed above, was deemed crucial.


### `voom`: Variance Modelling at Observational Level

A detailed description of the *voom* procedure is as follows. Firstly, for each gene, a linear model is fitted to its normalised log-*cpm* values, $y_{gi}$ by ordinary least squares. This linear model is of the form $E(y_{gi})=\mu_{gi}=x_i^T\beta_g$. Experimental design is incorporated into the model as the vector of covariates, given by $x_i$, can be used to describe the manner in which treatment factors are assigned to the various samples. $\beta_g$ represents a vector containing unknown coefficients which represent $\log_2$-fold changes between the conditions of interest. 

The application of these gene-wise linear models result in the production of regression coefficient estimates $(\hat\beta_g)$, together with fitted log-*cpm* values, $\hat\mu_{gi}=x_i^T\hat\beta_g$ and residual standard deviations, $s_g$. These fitted log-*cpm* values can be transformed into predicted counts by: $$\hat \lambda_{gi} = \hat \mu_{gi} + \log_2(R_i+1) - \log_2(10^6).$$
In addition, average log-*cpm* values, $\bar y_g$ are calculated for each gene and their conversion into an average number of log-counts can take place: $$\tilde r_g = \bar y_g + log_2(\tilde R) - log_2(10^6).$$ 
The geometric mean of library sizes added to 1 is denoted by $\tilde R$.  
 
The method proceeds by fitting a LOWESS (locally weighted scatterplot smoothing) curve to the square-root residual standard deviations, $\sqrt {s_g}$ as a function of the mean log-count, $\tilde r_g$ for each gene, in order to acquire a statistically robust mean-variance trend. It is because of their approximate symmetric distribution that square-root standard deviations are used. By interpolating this LOWESS curve between ordered $\tilde r_g$ values, the piecewise linear function, $\text{lo}()$ may be specified. This function, $\text{lo}()$ is used to predict the standard deviations of each indiviudal observation with the result of $\text{lo}(\hat \lambda_{gi})$ taken as equating to the square-root of the predicted standard deviation of $y_{gi}$. The *voom* precision weight for a specific observation is thus defined as its inverse predicted variance, $w_{gi} = \text{lo}(\hat \lambda_{gi})^{-4}$. Inclusion of weights into the linear modelling process effectively eradicates the existence of a mean-variance relationship in the log-*cpm* values. 
 
Log-*cpm* values $y_{gi}$ accompanied by their respective weights $w_{gi}$ may now be satisfactorily inputted into the `limma`'s linear modelling pipeline. In fact, many of the functions in `limma` can successfully facilitate quantitative weights. Hence, this method proposed by @law has opened up the possibility of performing thorough microarray-like analyses on RNA-Seq data, in which ignorance of the mean-variance relationship of log-*cpm* values at observational level is not required. 


The effect of the application of the `voom()` function to the dataset is highlighted in Figure \ref{fig:voom} below. To the left is a depiction of the original mean-variance relationship of the log-*cpm* values. This plot, known as the `voom`-plot, presents a steadily decreasing trend resulting from variation which has arisen from both technical and biological sources. As a consequence of fitting linear models to log-*cpm* values, residual variances are obtained. It is the quarter-root of these variances that can be seen to be plotted against the average log-*cpm* value for each gene. The second plot, generated using the `plotSA` function in `limma` is added to demonstrate the removal of the dependence of the variance on the mean. Similarly, square-root residual standard deviations are plotted against their corresponding mean log-*cpm* values. The residual standard deviations in this instance are obtained as a result of fitting linear models with `lmFit` and implementing `eBayes`, in which the *voom* precision weights are now taken into account. The horizontal blue line highlights the average of all square-root residual standard deviations. 


```{r voom, echo=FALSE, fig.align='center', fig.height=4.5, fig.width=7.5, out.width='80%', fig.cap='\\label{fig:voom}\\textit{For each gene, the mean, on the x-axis, is plotted against the square-root standard deviation, on the y-axis, before (left) and after (right) the voom function is applied.}'}
#LIMMA-VOOM
# specify a design matrix without intercept term
#sample_info$X[1:8] <- c("Secreted.EV","Secreted.EV","Secreted.EV","Secreted.EV","Cell","Cell","Cell","Cell")
#sample_info$X
#group <- factor(sample_info$X)
#group <- relevel(group, ref="Control")
#group

#fig.cap='Voom-plot, on the left with '
group = factor(sample_info_mouse$X)
design <- model.matrix(~group-1)
colnames(design) <- c("Cell","SecretedEV")

par(mfrow=c(1,2))
# normalise data
v <- voom(y,design,plot=TRUE)
fit <- lmFit(v)
cont.matrix <- makeContrasts(CellvsEV=Cell - SecretedEV,levels=design)

fit.cont <- contrasts.fit(fit,cont.matrix)
fit.cont <- eBayes(fit.cont)
plotSA(fit.cont, main="Final model: Mean-variance trend")
#head(v$E) # matrix of expression values ... 
# matrix of precision weights, each observation has individual weight
```






```{r, echo=FALSE,include=FALSE}
par(mfrow=c(1,2))
boxplot(log_counts,xlab="",ylab="Log2 counts per million",las=2,main="Unnormalised logCPM")
abline(h=median(log_counts),col="green",lwd=2)

boxplot(v$E,xlab="",ylab="Log2 counts per million",las=2,main="Voom transformed logCPM")
abline(h=median(v$E),col="orange",lwd=2)
```




## Results of the Analysis of RNA-Seq Data 

As previously stated, the log-*cpm* values accompanied by their *voom* precision weights are entered into the same linear modelling pipeline that has been described in significant detail for microarray data in Section 3. Upon completion of this process, a total of $164$ genes were considered to show evidence of differential expression, $74$ downregulated and $90$ upregulated, in which the contrast of interest was specified as `Cell-SecretedEV`. 

The summary statistics for the top $4$ differentially expressed genes, ordered according to their $p$-values, is included below. The `logFC` for a particular gene may be interpreted as the average of its log-*cpm* values in the two `SecretedEV` samples subtracted from the average of the two `RNACell` samples, while its `AveExpr` is intuitively its mean log-*cpm* value across all samples. The moderated $t$-statistics and their corresponding $p$-values have been computed as described in Section 3. The $p$-values have been adjusted using the BH method which corrects for multiple testing by controlling the false discovery rate. It can be seen from this table that if ranking was to occur with respect to their $B$-statistics, the genes would be ordered in a different manner. This attribute is due to the introduction of weights which has resulted in the unscaled standard deviations, $v_{gj}$ no longer being constant across the genes. Thus, it now cannot be deduced that the $B$-statistic, $B_{gj}$ is a monotonically increasing function of the absolute value of the moderated $t$-statistic, $|\tilde{t}_{gj}|$. 


```{r, echo=FALSE}
## DIFFERENTIAL EXPRESSION WITH LIMMA
table <- topTable(fit.cont,coef="CellvsEV",sort.by="p",number=164)
table[1:4,]
summa.fit <- decideTests(fit.cont)
# summary(summa.fit)
```

A volcano plot has been produced to highlight the genes that have been found to be statistically significant as a result of the performed analysis. The top 4 differentially expressed genes from the table above are named in this plot, which can be viewed in Figure \ref{fig:volcano}. The large $\log_2$-fold changes of these genes are evident, especially `GeneID62` and `GeneID195` which have `logFC` values of $-10.71$ and $-11.89$, respectively. As with all volcano plots, downregulated genes, coloured in red, lie to the extreme left of the plot while upregulated genes, coloured in green, can be seen on the right. 

```{r volcano, echo=FALSE, fig.align='center',out.width='65%',out.height='100%', fig.cap='\\label{fig:volcano}\\textit{Volcano plot depicting the results of the differential expression analysis performed on this local mouse dataset.}'}


#plotMD(fit.cont,coef=1,status=summa.fit[,"CellvsEV"],values=c(1,-1))
colour <- brewer.pal(8,"Paired")
new_names <- substr(rownames(fit.cont$coefficients),start=5,stop=10)
threshold.high <- sort(table$logFC, decreasing = TRUE)[20]
threshold.low <- sort(table$logFC, decreasing = FALSE)[20]
volcanoplot(fit.cont,coef=1,highlight=6, names=new_names, hl.col="navyblue",pch=20)
with(subset(table, logFC > 0),
    points(logFC, -log10(P.Value), pch=20, col=colour[4]),cex=0.2)
with(subset(table, logFC < 0),
     points(logFC, -log10(P.Value), pch=20, col=colour[6]),cex=0.2)
#abline(h=1.511271372)



#library(calibrate)
#par(mfrow=c(1,1))

#gene_names <- substr(rownames(fit.cont$coefficients),start=5,stop=10)
#threshold.high <- sort(table$logFC, decreasing = TRUE)[20]

#with(table, plot(logFC, -log10(P.Value), pch=20,
#     main="Volcano plot", xlim=c(-15,10)))
#with(subset(table, logFC > threshold.high),
#     points(logFC, -log10(P.Value), pch=20, col="red"))
#with(subset(table, logFC < threshold.low),
#     points(logFC, -log10(P.Value), pch=20, col="red"))
#with(subset(table, adj.P.Val<.0000000005 & abs(logFC)>1), textxy(logFC, -log10(P.Value), labs=gene_names, cex=.8))


#with(subset(table, logFC > threshold.high),
#     textxy(logFC, -log10(P.Value), labs=gene_names, cex=.8))
#with(subset(table, logFC < threshold.low),
#     textxy(logFC, -log10(P.Value), labs=gene_names, cex=.8))


#plotMD(fit.cont,coef=2,status=summa.fit[,"L.PregVsLac"],values=c(1,-1))
#  volcanoplot(fit.cont,coef=2,highlight=200)

```



# Hierarchical Clustering


## Introduction to Hierarchical Clustering

Hierarchical clustering is a method which has been used since the very beginning of microarray data analysis. Being an easily visualised and intuitive technique, it has proven to be most useful in performing tasks such as assessing similarity of gene expressions and ensuring that basic global patterns of the dataset in hand are as expected. It is generally assumed that similar patterns of expression are shown by technical and biological replicates while pattern of two different experimental conditions would be significantly more varied. 
Thus, the principal aim of hierarchical clustering is to provide definitive clustering which characterizes a set of genes or samples with respect to a given distance metric. It is referred to as an unsupervised method due to the fact that the number of clusters are not set in advance and there is no prior knowledge of experimental design. 

In the following, the task of clustering a set of samples will be considered. However, it is important to note that the clustering of genes into small sets is also a topic of significant interest. Clustering of genes is regularly used to assist in the identification of genes belonging to the same molecular pathway.
Before hierarchical clustering is performed, two important decisions are required:

* How should one compute the degree of similarity between pairs of samples?
* How should this concept of similarity be implemented in order to obtain clusters of samples?

A large variety of cluster analysis methodology exists, with the degree of sample similarity in many of these methods based on a specific definition of *distance* between a pair of samples. A popular approach for the evaluation of this concept of distance between microarray/RNA-Seq samples is that of the Pearson correlation coefficient, $r$, as discussed in @statsbio. This correlation coefficient is a measure of the strength of the linear relationship between two variables.  It is defined as the covariance of two variables divided by the product of standard deviations. Pairwise correlations are computed in `R` using the `cor()` function.
Given an estimate of the correlation, $r$, between expression levels of two genes, a distance between these genes may be consequently defined in various ways. If the sign of $r$ is deemed not to be of importance, a possible distance measure is d = 1 - $\mid r \mid$. This is exactly 0 when $r = \pm 1$, indicating that genes which are judged to be perfectly correlated are taken as having no distance between them. 
The Euclidean distance is also commonly used in order to measure the distance between two vectors of read counts. However, for RNA-Seq data, large differences in the library sizes of two samples can greatly affect the Euclidean distance. 

With this concrete definition for distance between samples, a choice must now be made as to how the inter-cluster distance is to be judged. As above, there are multiple options for linkage functions, with the most popular being: 

1. *complete-linkage*: distance between two clusters of samples is defined as the largest distance between any two 
neighbours. 
2. *average-linkage*: understood to have a very similar meaning to complete-linkage but now, the average distance between the individual components of the two clusters is considered.

Two samples are regarded as being neighbours if they belong to different clusters. It has been established that complete linkage often outperforms average linkage.

The result of this clustering technique is a complete tree with individual samples as leaves and the root as the convergence point of all branches. Unlike real trees, these hierarchical trees are drawn with a root on top and branches developing underneath. This diagram produced by hierarchical clustering is known as a dendrogram. It clearly illustrates a hierarchy of categories based on their degree of similarity. Pairwise comparisons of the individual samples allow for the grouping together of similar samples into clusters, resulting in the creation of the dendrogram. These comparisons can be made using a hierarchical clustering strategy which is either agglomerative or divisive in combination with specific distance and linkage functions. The method of tree construction which commences with individual genes and works up towards the root is agglomerative, more commonly known as the *bottom-up* approach. At the start, $n$ clusters are present, each consisting of a single sample. In the case where the clustering algorithm is being applied to the set of genes, then the $n$ clusters would each hold an individual gene. At each time point of the algorithm, the distances from each cluster to every other cluster are computed based on a specification of inter-cluster distance. Then, this hierarchical agglomerative clustering method repeatedly merges the two nearest clusters into a new single super-cluster until construction of the entire tree has taken place.

```{r cluster, echo=FALSE, fig.align='center',out.width='80%', fig.cap='\\label{fig:cluster}\\textit{Dendrogram obtained when samples from the leukemia dataset have been clustered according to correlation coefficients and complete-linkage is used to define inter-cluster distance.}'}
dist_corr <- as.dist(1-abs(cor(exprs(leukemiasEset),method="pearson")))
label_color <- brewer.pal(5,"Dark2")[as.numeric(pData(leukemiasEset)$LeukemiaType)]
myplclust(hclust(dist_corr),labels=pData(leukemiasEset)$LeukemiaType,lab.col=label_color,cex=0.6,main='')
```

By default, the `hclust` function in `R` uses the complete linkage method for hierarchical clustering clusters genes or samples using the agglomerative approach previously detailed. The dendrogram obtained upon use of this function for the $60$ samples present in the `leukemiasEset` dataset is included in Figure \ref{fig:cluster}. In this instance, the correlation coefficient, $r$ has been chosen as a suitable distance measure. The samples have been labelled according to their respective leukemia types. It is interesting to see that division into $5$ clear separate groups has not occurred. However, many similarly coloured samples seem to appear close to one another. Additionally, it is witnessed that all `CLL` samples are grouped alone in a distinctly defined cluster. 

Unfortunately, this technique of hierarchical clustering also has its limitations, with one of the most prominent being that it is not an iterative procedure. Thus, great sensitivity to noise, outliers and even, the different cluster distance measures exists. It is also noted that the process performs significantly better if implemented on a limited dataset, with less than approximately 300 genes or samples.  

    
## Heat Maps

Irrespective of which platform, microarrays or RNA-Seq, has been used to generate gene expression data, one of the most popular forms of visualisation of this data is through the production of a heat map. This heat map construction often relies on the clustering techniques detailed above as two separate dendrograms, one for samples and one for genes, are included in the image. The heat map exhibits the data in grid format, in which the different colours of the components of the grid are used to display the degree of change in expression for each gene across the range of samples. The genes are represented by the rows and the columns of the grid are used to represent the samples. This graphical representation of the data can make the identification of groups of genes and samples with common features much easier. 

The `gplots` package in `R` provides access to the creation of heat maps using the `heatmap.2` function. This function was employed to assemble the heat map included below in Figure \ref{fig:heatmap}. For demonstration purposes, the log-intensity data from two conditions, `AML` and `NoL` for the $100$ most variable genes across these $24$ samples from the `luekemiasEset` dataset has been used. Euclidean distances have been computed by `heatmap.2` in order to manufacture the two dendrograms seen on the outer edges of the diagram. In this plot, a bright orange colour symbolizes genes showing evidence for upregulation while darker green and blue shades suggest the presence of downregulated genes. 

Further inspection of the sample dendrogram suggests that making a cut at a certain height shows the possibility of defining three separate clusters of samples. All the `NoL` samples have been clearly clustered together on the right. This is supported by obvious dissimilarities in the colour palettes of the two halves of the heat map. Furthermore, it is of interest to note the behaviour of the group of four `AML` samples which have been regarded as being closer in distance to the cluster of `NoL` samples than to the other eight `AML` samples. This characteristic is also portrayed well in the heat map as there is an obvious lack of green in the corresponding columns of these `AML` samples. Overall, many downregulated genes, which are depicted by the green and blue colours, are visible on the left third of the heat map. This region corresponds to samples obtained from `AML` patients. This observation is accompanied by a strong orange signal being broadcast from the right of the heat map, indicating numerous upregulated genes in all $12$ `NoL` samples. 


```{r heatmap, echo = FALSE, out.width='80%',fig.align='center',fig.cap='\\label{fig:heatmap}\\textit{Heatmap produced using the 12 NoL and 12 AML samples and the 100 most variable genes across these samples from the leukemia dataset.}'}
# clustering genes etc. 
#e <- exprs(vdx)[1:100,]
#d <- dist(e)

#mypar()
#hc <- hclust(d)
#hc
#plot(hc,cex=0.5)
#myplclust(hc,labels=featureNames(vdx),cex=0.5)
#abline(h=120)
#hclusters <- cutree(hc,h=120)
#table(true=pData(vdx)[1:100,]$er,cluster=hclusters)


# clustering samples

# d <- dist(t(exprs(leukemiasEset)))


#myplclust(hclust(dist_corr),labels=pData(leukemiasEset)$LeukemiaType,lab.col=as.numeric(pData(leukemiasEset)$LeukemiaType),cex=0.6,main="Cluster Dendrogram with Correlation distance")




var_genes <- apply(exprs(leukemiasEset_Sub),1,var)
select_var <- names(sort(var_genes,decreasing=TRUE))[1:100]
# subset expression matrix
high_vary_genes <- exprs(leukemiasEset_Sub)[select_var,]
mypalette <- brewer.pal(11,"Spectral")
morecols <- colorRampPalette(mypalette)
#col.cell <- c("darkorange2","royalblue4","green","purple","red")[leukemiasEset$LeukemiaType]
select_var <- str_sub(select_var,10,15)
heatmap.2(high_vary_genes,col=rev(morecols(10)),trace="column",main=" ",labCol= colnames(leukemiasEset_Sub), cexCol = 0.7, cexRow = 0.6, labRow = select_var,scale="row")




#hmcol <- colorRampPalette(brewer.pal(9,"GnBu"))(100)
#rv <- rowVars(exprs(vdx))
#idx <- order(-rv)[1:100]
# heatmap.2 function recommended, add colors to indicate tissue on top:
#cols <- palette(brewer.pal(8,"Dark2"))
#head(cbind(colnames(exprs(vdx)),cols))
#heatmap.2(exprs(vdx)[idx,],labCol=pData(vdx)$samplename,trace="none")


```


In a similar manner, a heatmap has been generated for the local RNA-Seq mouse dataset. A more simplistic heatmap can thus be viewed in Figure \ref{fig:heatmap2}, which contains only 4 samples. The log counts of the $264$ genes which were retained after filtration have been employed to produce this image. The sample dendrogram on top has clearly divided the samples into two groups as expected. The `RNACell` samples are positioned to the left while the `SecretedEV` are on the right. Clear similarities can be seen in the colour profiles of the two samples in each group. In this heatmap, the colour red is representative of upregulated genes while blue emphasizes downregulated genes. In several instances, a band of red can be seen on one half of the plot with shades of blue visible in the section of the other half corresponding to this band. Therefore, upon inspection of this heatmap, it is evident that there exist great differences in gene expression patterns between the two groups of the dataset.


```{r heatmap2,echo=FALSE, out.width='80%', fig.align='center',fig.cap='\\label{fig:heatmap2}\\textit{Heatmap produced using the genes which have been retained after filtration and all four samples in the local mouse RNA-Seq dataset.}'}
mypalette <- brewer.pal(11,"Spectral")
morecols <- colorRampPalette(mypalette)
col.cell <- c("blue","red","blue")[sample_info_mouse$X]
heatmap.2(log_counts,col=rev(morecols(50)),trace="column",main=" ", ColSideColors=col.cell,scale="row",cexCol=1)
```








```{r,echo=FALSE,include=FALSE}
#var_genes <- apply(log_counts,1,var)

#select_var <- names(sort(var_genes,decreasing=TRUE))

# subset logcounst matrix
#high_variable_lcpm <- log_counts[select_var,]
#head(high_variable_lcpm)



#select_var2 <- names(sort(var_genes,decreasing=FALSE))[1:500]
#low_variable_logcpm <- logcounts[select_var2,]
#mypalette2 <- brewer.pal(11,"RdYlBu")
#morecols2 <- colorRampPalette(mypalette2)
#col.cell2 <- c("purple","orange","red")[sampleinfo$Status]
#heatmap.2(low_variable_logcpm,col=rev(morecols2(50)),trace="column",main="Top 500 least variable genes across samples", ColSideColors=col.cell2,scale="row")


# related to approach discussed in statistical methods in bioinformatics
distance.log <- as.dist(1-abs(cor(log_counts,method="pearson")))

plot(hclust(distance.log),labels=colnames(log_counts),main="log transformed read counts\ndistance: Pearson correlation")
plot(hclust(distance.log,method="average"),labels=colnames(log_counts),main="log transformed read counts\ndistance: Pearson correlation") # this is really the exact same as above
#yh = 0.03
#lines(c(0,12),c(0.03,0.03))


distance.log2 = dist(t(log_counts),method="euc")
plot(hclust(distance.log2),labels=colnames(log_counts),main="log transformed read counts\ndistance: Euclidean")
## transpose depending on gene/samples etc.
# complete linkage function used here
```



